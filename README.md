# ğŸ” RAG Chatbot â€“ FastAPI + Streamlit + Groq

Retrievalâ€‘Augmented Generation (RAG) chatbot that lets you **chat with your own PDFs and text files**.  
Documents are ingested, chunked, embedded, indexed with FAISS, and queried via a FastAPI backend with a Streamlit chat UI on top.

---

## âœ¨ Features

- **Domainâ€‘agnostic RAG**  
  Ingests any PDFs/TXT from `data/pdf_files/` and `data/text_files/` and answers questions over that corpus.

- **Modular RAG pipeline**
  - Document ingestion with LangChain loaders (PDF + text).
  - Recursive character chunking.
  - SentenceTransformers embeddings.
  - FAISS innerâ€‘product index for similarity search.
  - Custom `RAGRetriever` abstraction and LLM wrapper. 

- **Fullâ€‘stack app**
  - **FastAPI** backend exposing:
    - `POST /query` â€“ ask a question, get an answer + supporting contexts.
    - `POST /upload_docs` â€“ upload new PDFs/TXT and rebuild the index.
    - `GET /stats` â€“ see corpus statistics (docs, chunks, sources).
  - **Streamlit** frontend:
    - Chatâ€‘style interface (`st.chat_message`) for conversational queries.
    - Sidebar with corpus overview and document upload.

- **LLM integration (Groq)**
  - Uses Groqâ€™s `llama-3.1-8b-instant` via LangChainâ€™s `ChatGroq` for fast, instructionâ€‘following responses grounded in retrieved context. 

- **Evaluation**
  - Offline evaluation script computing **precision@5** (and recall variants) on a small labeled question set to verify retrieval quality. 

- **Designed for resumes & real projects**
  - Clean, modular structure: `rag_files/` (engine), `api/` (service), `ui/` (UI).
  - Easy to adapt to any domain: research papers, course notes, internal docs, etc.

---

## ğŸ§± Architecture Overview

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚    Documents       â”‚
      â”‚  (PDF, TXT, â€¦)     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Data Ingestion      â”‚
    â”‚  (LangChain loaders) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Chunking           â”‚
    â”‚ Recursive splitter   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Embeddings         â”‚
    â”‚ SentenceTransformers â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Vector Store       â”‚
    â”‚  FAISS (IP index)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   RAG Retriever      â”‚         â”‚     LLM (Groq)       â”‚
    â”‚  topâ€‘k relevant      â”‚ â”€â”€â”€â”€â”€â–¶  â”‚ ChatGroq, prompt     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                  â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
                  Final Answer (FastAPI â†’ Streamlit UI)


**Backâ€‘end:** `rag_files/` + `api/`  
**Frontâ€‘end:** `ui/streamlit_app.py`  

---

## ğŸ“‚ Project Structure

```text
RAG_Chatbot/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py           # FastAPI app: /query, /upload_docs, /stats
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py  # Streamlit chat UI + uploads + corpus overview
â”œâ”€â”€ rag_files/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_ingestion.py # PDF/TXT loaders
â”‚   â”œâ”€â”€ chunking.py       # RecursiveCharacterTextSplitter
â”‚   â”œâ”€â”€ embeddings.py     # EmbeddingManager (SentenceTransformers)
â”‚   â”œâ”€â”€ vector_store.py   # VectorStore (FAISS IP index, docs)
â”‚   â”œâ”€â”€ retriever.py      # RAGRetriever
â”‚   â”œâ”€â”€ llm.py            # ChatGroq wrapper + prompt
â”‚   â”œâ”€â”€ pipeline.py       # RAGPipeline + get_pipeline + answer_question
â”‚   â””â”€â”€ eval.py           # precision@k (and recall) evaluation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdf_files/        # user PDFs (gitignored or small samples)
â”‚   â””â”€â”€ text_files/       # user TXT files
â”œâ”€â”€ .env.example          # example env file (no secrets)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

---
## âš™ï¸ Setup & Installation

### 1. Clone the repository

```bash
git clone https://github.com/ritikaugale/RAG_Chatbot.git
cd RAG_Chatbot
```

### 2. Create and activate a virtual environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

```bash
# Set your Groq key
cp .env.example .env
# Edit .env and set:
GROQ_API_KEY=your_api_key_here
```

### 5. Prepare data folders

```bash
mkdir -p data/pdf_files data/text_files
```

---

## ğŸš€ Running the App

Run the backend and frontend in **separate terminals** (same virtual environment).

### 1. Start the FastAPI backend

```bash
uvicorn api.main:app --reload
```

### 2. Start the Streamlit frontend

```bash
streamlit run ui/streamlit_app.py
```

---

## ğŸ’¬ Using the Chatbot

### 1. Inspect the Corpus

In the Streamlit sidebar, **Corpus Overview** shows:

- Number of documents  
- Number of chunks  
- List of source paths (PDF/TXT filenames)  

---

### 2. Upload Your Own Documents

1. Use **â€œUpload PDFs or TXTâ€** in the sidebar  
2. Select one or more `.pdf` / `.txt` files  
3. Click **â€œSave uploaded filesâ€**  

The backend will:
- Save them into `data/pdf_files` / `data/text_files`
- Rebuild the FAISS index on the next query  

---

### 3. Ask Questions

Type a question in the chat input  
Example:

> What is the SSD algorithm used for?

The app will:

1. Embed your query  
2. Retrieve top-k relevant chunks via FAISS  
3. Send context + question to the Groq LLM  
4. Stream the answer back to the chat UI  

---

### 4. Show Retrieved Contexts

Enable **â€œShow retrieved contextsâ€** to inspect the exact chunks used for answer generation.

---

### 5. Tune Retrieval

Adjust the **Top-K contexts slider** in the sidebar (e.g., from 3 to 8).

- Higher `k` â†’ better recall  
- Higher `k` â†’ more noise + longer responses  

---

## ğŸ“Š Evaluation (Precision@k)

### 1. Prepare `qrels.json`

Create a `qrels.json` file as shown in the example.

### 2. Run evaluation

```bash
python -m rag_files.eval
```

---

## ğŸ“„ License

MIT License
