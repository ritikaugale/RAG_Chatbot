# ğŸ” RAG Chatbot â€“ FastAPI + Streamlit + Groq

Retrievalâ€‘Augmented Generation (RAG) chatbot that lets you **chat with your own PDFs and text files**.  
Documents are ingested, chunked, embedded, indexed with FAISS, and queried via a FastAPI backend with a Streamlit chat UI on top.

---

## âœ¨ Features

- **Domainâ€‘agnostic RAG**  
  Ingests any PDFs/TXT from `data/pdf_files/` and `data/text_files/` and answers questions over that corpus.

- **Modular RAG pipeline**
  - Document ingestion with LangChain loaders (PDF + text).
  - Recursive character chunking.
  - SentenceTransformers embeddings.
  - FAISS innerâ€‘product index for similarity search.
  - Custom `RAGRetriever` abstraction and LLM wrapper. 

- **Fullâ€‘stack app**
  - **FastAPI** backend exposing:
    - `POST /query` â€“ ask a question, get an answer + supporting contexts.
    - `POST /upload_docs` â€“ upload new PDFs/TXT and rebuild the index.
    - `GET /stats` â€“ see corpus statistics (docs, chunks, sources).
  - **Streamlit** frontend:
    - Chatâ€‘style interface (`st.chat_message`) for conversational queries.
    - Sidebar with corpus overview and document upload.

- **LLM integration (Groq)**
  - Uses Groqâ€™s `llama-3.1-8b-instant` via LangChainâ€™s `ChatGroq` for fast, instructionâ€‘following responses grounded in retrieved context. 

- **Evaluation**
  - Offline evaluation script computing **precision@5** (and recall variants) on a small labeled question set to verify retrieval quality. 

- **Designed for resumes & real projects**
  - Clean, modular structure: `rag_files/` (engine), `api/` (service), `ui/` (UI).
  - Easy to adapt to any domain: research papers, course notes, internal docs, etc.

---

## ğŸ§± Architecture Overview

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚    Documents       â”‚
      â”‚  (PDF, TXT, â€¦)     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Data Ingestion      â”‚
    â”‚  (LangChain loaders) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Chunking           â”‚
    â”‚ Recursive splitter   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Embeddings         â”‚
    â”‚ SentenceTransformers â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Vector Store       â”‚
    â”‚  FAISS (IP index)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   RAG Retriever      â”‚         â”‚     LLM (Groq)       â”‚
    â”‚  topâ€‘k relevant      â”‚ â”€â”€â”€â”€â”€â–¶  â”‚ ChatGroq, prompt     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                  â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
                  Final Answer (FastAPI â†’ Streamlit UI)


**Backâ€‘end:** `rag_files/` + `api/`  
**Frontâ€‘end:** `ui/streamlit_app.py`  

---

## ğŸ“‚ Project Structure

```text
RAG_Chatbot/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py           # FastAPI app: /query, /upload_docs, /stats
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py  # Streamlit chat UI + uploads + corpus overview
â”œâ”€â”€ rag_files/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_ingestion.py # PDF/TXT loaders
â”‚   â”œâ”€â”€ chunking.py       # RecursiveCharacterTextSplitter
â”‚   â”œâ”€â”€ embeddings.py     # EmbeddingManager (SentenceTransformers)
â”‚   â”œâ”€â”€ vector_store.py   # VectorStore (FAISS IP index, docs)
â”‚   â”œâ”€â”€ retriever.py      # RAGRetriever
â”‚   â”œâ”€â”€ llm.py            # ChatGroq wrapper + prompt
â”‚   â”œâ”€â”€ pipeline.py       # RAGPipeline + get_pipeline + answer_question
â”‚   â””â”€â”€ eval.py           # precision@k (and recall) evaluation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdf_files/        # user PDFs (gitignored or small samples)
â”‚   â””â”€â”€ text_files/       # user TXT files
â”œâ”€â”€ .env.example          # example env file (no secrets)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


## âš™ï¸ Setup & Installation

## Setup

1. Clone the repository

'''bash
git clone https://github.com/ritikaugale/RAG_Chatbot.git
cd RAG_Chatbot

2. Create and activate a virtual environment
'''bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

3. Install dependencies
'''bash
pip install -r requirements.txt

4. Configure environment variables

# set your Groq key
cp .env.example .env  # if you create an example file
# edit .env and set GROQ_API_KEY=...

5. Prepare data folders 
'''bash
mkdir -p data/pdf_files data/text_files

## ğŸš€ Running the App
You run the backend and frontend in separate terminals (same venv).

1. Start the FastAPI backend

'''bash
uvicorn api.main:app --reload


2. Start the Streamlit frontend

'''bash
streamlit run ui/streamlit_app.py


## Using the Chatbot

1. Inspect the corpus
In the Streamlit sidebar:
    â€œCorpus overviewâ€ shows:

            Number of documents.
            Number of chunks.
            List of source paths (PDF/TXT filenames). 

2. Upload your own documents

        Use â€œUpload PDFs or TXTâ€ in the sidebar.
        Select one or more .pdf / .txt files.
        Click â€œSave uploaded filesâ€.
        The backend saves them into data/pdf_files / data/text_files and rebuilds the index on the next question.

3. Ask questions

    Type a question in the chat input (e.g. â€œWhat is the SSD algorithm used for?â€).

    The app:

        Embeds your query.
        Retrieves topâ€‘k relevant chunks via FAISS.
        Sends context + question to the Groq LLM.
        Streams the answer back to the chat UI.

4. Use â€œShow retrieved contextsâ€ to see exactly which snippets were used. 

5. Tune retrieval

        Adjust Top K contexts slider in the sidebar (e.g. from 3 to 8).
        Higher k = more context, potentially better recall but more noise.


## ğŸ“Š Evaluation (Precision@k)

1. Prepare a qrels.json as given in example

2. Run evaluation
'''bash
python -m rag_files.eval



## ğŸ“„ License

MIT licence


